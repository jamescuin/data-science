{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8976535",
   "metadata": {},
   "source": [
    "# BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a894c4",
   "metadata": {},
   "source": [
    "The Bilingual Evaluation Understudy Score, or BLEU for short, is used to evaluate a generated sentence against a reference sentence. \n",
    "\n",
    "A perfect match gives a score of 1.0, whereas a perfect mismatch gives a score of 0.0. \n",
    "\n",
    "The metric was developed for evaluating the predictions made by Neural Machine Translation (NMT) models. Although not perfect, it offers 5 benefits:\n",
    "* It is quick and easy to calculate.\n",
    "* It is easily understandable.\n",
    "* It is language independent.\n",
    "* It correlates highly with human evaluation.\n",
    "* It has been widely adopted. (For example in 'Attention is All You Need'\n",
    "\n",
    "The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where a 1-gram (or unigram) would be each token and a bigram comparison would be each token pair. The comparison is made regardless of word order. \n",
    "\n",
    "The counting of matching n-grams is modified to ensure that it takes the occurrence of the words in the reference text into account, not rewarding cadidate translation that generates an abundance of reasonable words. This is often referred to as modified n-gram precision.\n",
    "\n",
    "A perfect score is not possible in practise as a translation would have to macth the reference exactly, which is not even possible by human translators in most cases. The number and quality of the references used to calculate the BLEU score is clearly a key factor, and means comparing scores across datasets can be troublesome. \n",
    "\n",
    "In addition to translation, we can use the BLEU score for other language generation problems with deep learning methods such as:\n",
    "\n",
    "* Language generation.\n",
    "* Image caption generation.\n",
    "* Text summarization.\n",
    "* Speech recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec97d3f",
   "metadata": {},
   "source": [
    "The NLTK provides an implementation of the BLEU score that can be used to evaluate generated text against a reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c426490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f399a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
